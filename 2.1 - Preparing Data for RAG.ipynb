{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5892be3e-9beb-44f9-ba5c-d400b9039ba1",
   "metadata": {},
   "source": [
    "## 2.1 - Preparing Data for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cee9ca2-15d9-4d4a-a75a-afbdaa24368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the arrow batch size as our PDF can be big in memory\n",
    "Spark. con. set(\"spark. sql. execut on. arrow. naxRecordsPerBatch, 10) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58e82e-1a9f-41db-9af8-830c1ac5758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles path = f\"{DA.paths.datasets}/arxiv-articles/\"\n",
    "table name = f\"{DA catalog_name. {DA.schema_name}. pdf_raw_text‚Äù\n",
    "\n",
    "# read df files\n",
    "df=(\n",
    "    spark.read.format (\"binary file\")\n",
    "    .option(\"recursiveFileLookup, \"true\")  \n",
    "    .load(articles_path)\n",
    "    )\n",
    "# save List of the files to table\n",
    "df.write.mode (\"overwrite\").saveAsTable(table_name)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da58b8-5db3-479c-ae61-202f284f70d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{articles_path.replace('dbfs:','/dbfs/')}2302.06476.pdf\", mode=\"rb\") as pdf:\n",
    "    doc = extract_doc_text(pdf.read())\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e96b8a-a56e-4350-88b3-1f08118ac33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index import Document, set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col,udf,length,pandas_udf,explode\n",
    "from unstructured.partition.auto import partition\n",
    "\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunk(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #set llama2 as tokenizer\n",
    "    set_global_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    #Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    def extract_and_split(b):\n",
    "        txt = extract_doc_text(b)\n",
    "        nodes = splitter.get_nodes_from_documents([Documents(text=txt)])\n",
    "        return [n.text for n in nodes]\n",
    "        \n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b70da-2865-4185-8f6f-04f3c3cf122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def_chunks = (df\n",
    "                  .withColumn(\"content\", explode(read_as_chunk(\"content\")))\n",
    "                  .selectExpr('path as pdf_name','content')\n",
    "              }\n",
    "display(df_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179bccfc-91fc-4dd6-9663-eee5c61532df",
   "metadata": {},
   "source": [
    "## How to Use Foundation model API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7185cf-284e-438b-b5cb-3a5b6b1e069e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "# bgee-large-en Foundation models are available using the /serving-endpoints/databricks-bge-large-en/invocationsapi.\n",
    "deploy_client = get_deploy_client(\"databricks\")\n",
    "\n",
    "#NOTE: if you change your embeddings model here, make sure you change it in the query step too\n",
    "embeddings = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [\"What is Apache Spark?\"]})\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df14645-7454-46d6-bb00-350e6f05a342",
   "metadata": {},
   "source": [
    "## Compute Chunking Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eba2a9-7c00-483a-8985-06f3527fe645",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    import mlflow.deployments\n",
    "    deploy_clients = ml.flow.deployments.get_deploy_client(\"databricks\")\n",
    "    def get_embeddings(batch):\n",
    "        # Note: this will fail id an excecution is thrown during emedding creation (add try/except if needed)\n",
    "        response =deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": batch})\n",
    "\n",
    "# Splitting the contents into batches of 150 items each, since the embedding model takes ar most 150 inputs oer request.\n",
    "max_batch_size =150\n",
    "batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "#Process each batch and collect the results\n",
    "all_embeddings = []\n",
    "for batch in batches:\n",
    "    all_embeddings += get_embeddings(batch.tolist())\n",
    "\n",
    "return pd.Series(all_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663ec32-449c-4bb9-8f6e-efe176191828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_chunk_emd = (df_chunks\n",
    "                .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "                .selectExpr('pdf_name', 'content', 'embedding')\n",
    "display(df_chunk_emd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e8b54-f702-436b-a49a-a72b396249c3",
   "metadata": {},
   "source": [
    "## Save Embeddings to a Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb067797-5480-47fe-9744-6ac9b369adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS pdf_text_embeddings (\n",
    "    id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "    pdf_name STRING,\n",
    "    content STRING,\n",
    "    embedding ARRAY <FLOAT>\n",
    "    --Note: The table has to be CDC beacuase VectorSearch is using DLT that is requiring CDC state\n",
    ") TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794307f-a930-4095-87f5-940826b61797",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_table_name = f\"{DA.catalog_name}.{DA.schema_name}.pdf_text_embeddings\"\n",
    "df_chunk_emd.write.mode(\"append\").saveAsTable(embedding_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429ba5bf-4361-4e7a-8f10-492adadb2fee",
   "metadata": {},
   "source": [
    "## Setup a Vector Search Endpoint\n",
    "Create self-managed vector search index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c48814-0768-4b40-b4b9-a82d114e6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign vs search endpoint by username\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "vs_endpoint_fallback = \"vs_endpoint_fallback\"\n",
    "vs_endpoint_name = vs_endpoint_prefix+\"7\"\n",
    "# vs_endpoint_name = vs_endpoint_prefix+str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "\n",
    "print(f\"Vector Endpoint name: {vs_endpoint_name}. In case of any issues, replace variable 'vs_endpoint_name' with\n",
    "'vs_endpoint_fallback' in demos and labs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f7c976-4cf7-4939-86b4-6a7966f4939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from databricks.sdk import WorkspaceClient\n",
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "vsc = VectorSearchClient(disable_notice = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f426b4d-9056-436f-90ef-8770e1d81bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the status of the endpoint\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, vsc_endpoint_name)\n",
    "print(f\"Endpoint named {vs_endpoint_name} is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908eeb6e-d49b-4844-a45a-c6322f6f2b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The table we'd like to index\n",
    "source_table_fullname = f\"{DA.catalog_name},{DA.schema_name}.pdf_text_embeddings\"\n",
    "\n",
    "# where we want to store our index\n",
    "vs_index_fullname = f\"{DA.catalog_name}.{DA.schema_name}.pdf_text_self_managed_vs_index\"\n",
    "\n",
    "# create or sync the index\n",
    "if not index_exists(vsc, vs_endpoint_name, vs_index_fullname):\n",
    "    print(f\"Creating index {vs_index_fullname} on endpoint {vs_endpoint_name}...\")\n",
    "    vsc.create_delta_sync_index(\n",
    "        endpoint_name = vs_endpoint_name,\n",
    "        index_name = vs_index_fullname,\n",
    "        source_table_name = source_table_fullname,\n",
    "        pipeline_type = \"TRIGGERED\", #Sync needs to be manually triggered\n",
    "        primary_key = \"id\",\n",
    "        embedding_dimensions=1024, #Match your model embedding size (bge)\n",
    "        embedding_vector_column = \"embedding\"\n",
    "    )\n",
    "else:\n",
    "    #Trigger a sync to update our vs content with the nwe data saved in the table\n",
    "    vsc.get_index(vs_endpoint_name, vs_index_fullname).sync()\n",
    "#Let's wait for the index to be ready and all our embeddings to be created and indexed\n",
    "wait_for_index_to_be_ready(vsc, vs_endpoint_name, vs_index_fullname)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4018384-bd86-477b-825e-bf1b5b5df4ff",
   "metadata": {},
   "source": [
    "## Search for similar content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a42eb1-7998-46e5-9221-00200d03c1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.deployments\n",
    "\n",
    "deploy_clients = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "question = \"How Generative AI impacts humans?\"\n",
    "response = deploy_client.predict(endpoint=\"databricks-bge-large-en\", inputs={\"input\": [question]})\n",
    "embeddings = [e['embedding'] for e in response.data]\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb81032-23dc-483d-9edc-7b7812080c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get similar 5 documents\n",
    "results = vsc.get_index(vs_endpoint_name, vs_index_fullname).similarity_search(\n",
    "    query_vector = embeddings[0],\n",
    "    columns = [\"pdf_name\", \"content\"],\n",
    "    num_results = 5)\n",
    "#format result to align with remainder lib format\n",
    "passages =[]\n",
    "for doc in results.get('result', {}).get('data_array',[]):\n",
    "    new_doc = {\"file\": doc[0[, \"text\":doc[1]}\n",
    "    passages.append(new_doc)\n",
    "print(passages)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f9409d-c631-4887-8f90-bbc9d8de113a",
   "metadata": {},
   "source": [
    "## Re-ranking search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0009b-9141-439c-a000-c2f3fcd051b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashrank import Ranker, RerankRequest\n",
    "\n",
    "ranker = Ranker(model_name = \"rank-T5-flan\", cache_dir=f\"{DA.paths.working_dir.replace('dbfs:/','/dbfs/')}/opt\")\n",
    "\n",
    "rerankrequest = RerankRequest(query=question, passages=passages)\n",
    "results = ranker.rerank(rerankrequest)\n",
    "print(*resultss[:3], sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7790c-6e5a-4c9b-b493-61c69c609b87",
   "metadata": {},
   "source": [
    "## Assembling and evaluating a RAG application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1251d147-7394-4427-82c7-80d5287fe492",
   "metadata": {},
   "source": [
    "# Set up the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf5a88-90b8-4986-9670-5f8e8528b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# components we created before\n",
    "# assign vs search endpoint by username\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "vs_endpoint_fallback = \"vs_endpoint_fallback\"\n",
    "# vs_endpoint_name = vs_endpoint_prefix+str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "vs_endpoint_name = vs_endpoint_prefix+\"7\"\n",
    "\n",
    "print(f\"Vector Endpoint name: {vs_endpoint_name}. In case of any issues, replace variable 'vs_endpoint_name with 'vs_endpoint_fallback' in demos and labs/\")\n",
    "vs_index_fullname = f\"{DA.catalog_name}.{DA.schema_name}.pdf_text_self_managed_vs_index\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70963e3a-71e6-47ae-9c4d-17831134ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "\n",
    "#Test embedding Langchain model\n",
    "# NOTE: your question embedding model must match the one used in the chunk in the prevous model\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "print(f\"Test embeddings: {embeddings_model.embed_query('What is GenerativeAI?')[:20]...\")\n",
    "\n",
    "def get_retriever(persist_dir:str=None):\n",
    "    # Get the vector search index\n",
    "    vsc = VectorSearchClient()\n",
    "    vs_index = vsc.get_index(\n",
    "        endpoint_name=vs_endpoint_name,\n",
    "        index_name = vs_index_fullname\n",
    "    )\n",
    "    # Create the retriever\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column = \"content\", embedding=embedding_model\n",
    "    )\n",
    "    # k defines the top k documents to retrieve\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "#test our retreiver\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.invoke(\"How Generative AI impacts humans?\")\n",
    "print(f\"Relevant documents: {similar_documents}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f9fcb-0aa1-41da-a0d9-d25171dc2b38",
   "metadata": {},
   "source": [
    "## Setup the Foundation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f84568a-d200-4529-bb06-8cc8f8a8d5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "#Test Databricks Foundation LLm model\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-llama-2-70b-chat\",max_tokens = 300)\n",
    "print(f\"Test chat model: {chat_model.invoke('What is Generative AI?')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e24103-2280-4b71-a88c-75d9e79f1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "TEMPLATE = \"\"\"You are a assistant for GENAI teaching class. You are answering questions realated to Generative AI\n",
    "and how it impacts human life. If the question is nor reated to one of these topics, kindly decline to ansert.\n",
    "If you don't know the answer, just say that ou don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
    "Use the following pieces of context to answer the qeustion at the end:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=TEMPLATE, input_variables=[\"context\", \"question\"])\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=get_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd663b0-2e5e-482d-9a8f-5ac711e9761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = {\"query\": \"How does Generative AI impact humans?\")\n",
    "answer = chain.invoke(question)\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77869df6-0d0f-441c-972b-1f666ad077ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ee8752-96ca-45ad-86c7-c9f543710152",
   "metadata": {},
   "source": [
    "## Evaluating the RAG pipeline\n",
    "### Prepare the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3d3c8-0370-48d3-b076-4ae9b2c0b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "eval_set = \"\"\"question, ground_truth, evolution_type, episode_done\n",
    "\"What are the limitations of symbolic planning in task and motion planning, and how can leveraging large language models help overcome these limitations?\", \"Symbolic planning in task and motion planning can be limited by the need for explicit primitives and constraints. Leveraging large language models can help overcome these limitations by enabling the robot to use language models for planning and execution, and by providing a way to extract and leverage knowledge from large language models to solve temporally extended tasks.\", simple, TRUE\n",
    "\"What are some techniques used to fine-tune transformer models for personalized code generation, and how effective are they in improving prediction accuracy and preventing runtime errors? \", \"The techniques used to fine-tune transformer models for personalized code generation include fine-tuning transformer models, adopting a novel approach called Target Similarity Tuning (TST) to retrieve a small set of examples from a training bank, and utilizing these examples to train a pretrained language model. The effectiveness of these techniques is shown in the improvement in prediction accuracy and the prevention of runtime errors.\", simple, TRUE How does the PPO-ptx model mitigate performance regressions in the few-shot setting?, \"The PPO-ptx model mitigates performance regressions in the few-shot setting by incorporating pre-training and fine-tuning on the downstream task. This approach allows the model to learn generalizable features and adapt to new tasks more effectively, leading to improved few-shot performance.\", simple, TRUE\n",
    "How can complex questions be decomposed using successive prompting?, \"Successive prompting is a method for decomposing complex questions into simpler sub-questions, allowing language models to answer them more accurately.\n",
    "This approach was proposed by Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner in their paper\n",
    "'Successive Prompting for Decomposing Complex Questions', presented at EMNLP 2022.\", simple, TRUE\n",
    "\"Which entity type in Named Entity Recognition is likely to be involved in information extraction, question answering, semantic parsing, and machine translation?\", Organization, reasoning, TRUE What is the purpose of ROUGE (Recall-Oriented Understudy for Gisting Evaluation) in automatic evaluation methods?,\n",
    "manipulation include integrating natural language understanding with reinforcement learning, understanding natural language directions for robotic navigation, and mapping instructions and visual observations to actions with reinforcement learning.\", simple, TRUE\n",
    "\"How does chain of thought prompting elicit reasoning in large language models, and what are the potential applications of this technique in neural text generation and human-AI interaction?\", \"The context discusses the use of chain of thought prompting to elicit reasoning in large language models, which can be applied in neural text generation and human-AI interaction. Specifically, researchers have used this technique to train language models to generate coherent and contextually relevant text, and to create transparent and controllable human-AI interaction systems. The potential applications of this technique include improving the performance of language models in generating contextually appropriate responses, enhancing the interpretability and controllability of AI systems, and facilitating more effective human-AI collaboration.\", simple, TRUE\n",
    "\"Using the given context, how can the robot be instructed to move objects around on a tabletop to complete rearrangement tasks?\", \"The robot can be instructed to move objects around on a tabletop to complete rearrangement tasks by using natural language instructions that specify the objects to be moved and their desired locations. The instructions can be parsed using functions such as parse_obj_name and parse_position to extract the necessary information, and then passed to a motion primitive that can pick up and place objects in the specified locations. The get_obj_names and get_obj_pos APIs can be used to access information about the available objects and their locations in the scene.\", reasoning, TRUE\n",
    "\"How can searching over an organization's existing knowledge, data, or documents using LLM-powered applications reduce the time it takes to complete worker activities?\", \"Searching over an organization's existing knowledge, data, or documents using LLM-powered applications can reduce the time it takes to complete worker activities by retrieving information quickly and efficiently. This can be done by using the LLM's capabilities to search through large amounts of data and retrieve relevant information in a short amount of time.\", simple, TRUE\n",
    "\"\"\"\n",
    "objStringI0(eval_set) \n",
    "eval_df pd.read_csv(obj)\n",
    "display(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ff0aa-b91b-42c6-b328-0306252b6358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "test_questions = eval_df [\"question\"].values.tolist()\n",
    "test_groundtruths = eval_df [\"ground_truth\"].values.tolist()\n",
    "\n",
    "answers = []\n",
    "contexts = []\n",
    "\n",
    "# answer each question in the dataset\n",
    "for question in test_questions:\n",
    "    # save the answer generated\n",
    "    chain_response chain.invoke({\"query\" question})\n",
    "    answers.append(chain_response [\"result\"])\n",
    "    \n",
    "    #save the contexts used\n",
    "    vs_response = vectorstore.invoke(question)\n",
    "    contexts.append(list(map(lambda doc: doc.page_content, vs_response)))\n",
    "    \n",
    "# construct the final dataset\n",
    "response_dataset = Dataset.from_dict({\n",
    "    \"inputs\": test_questions,\n",
    "    \"answer\": answers,\n",
    "    \"context\": contexts,\n",
    "    \"ground_truth\": test_groundtrutes\n",
    "})\n",
    "\n",
    "display(response_dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c553ef-66c7-4ba9-a43f-59eebcadc65b",
   "metadata": {},
   "source": [
    "## calculate evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8eb44d-81cc-4955-8ef0-45e02bd73bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import set_deployments_target\n",
    "\n",
    "set_deployments_target(\"databricks\")\n",
    "\n",
    "dbrx_answer_Isimilarity = mlflow.metrics.genai.answer_similarity(\n",
    "    model=\"endpoints:/databricks-dbrx-instruct\"\n",
    ")\n",
    "\n",
    "dbrx_relevance = mlflow.metrics.genai.relevance(\n",
    "    model=\"endpoints:/databricks-dbrx-instruct\"\n",
    ")\n",
    "\n",
    "results = mlflow.evaluate(\n",
    "    data=response_dataset.to_pandas(),\n",
    "    targets=\"ground_truth\",\n",
    "    predictions=\"answer\",\n",
    "    extra_metrics=[dbrx_answer_similarity, dbrx_relevance],\n",
    "    evaluators=\"default\",\n",
    ")\n",
    "\n",
    "display(results.tables['eval_results_table'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68944e8a-b516-4c14-b5f1-3105c2354a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import langchain\n",
    "\n",
    "# set model registery to UC\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = f{DA.catalog_name}. {DA.schema_name}.rag_app_demo4\"\n",
    "\n",
    "with mlflow.start_run(run_name=\"rag_app_demo4\") as run:\n",
    "    signature infer_signature (question, answer)\n",
    "    model_info = mlflow.langchain.log_model(\n",
    "        chain,\n",
    "        loader_fn=get_retriever,\n",
    "        artifact_path=\"chain\",\n",
    "        registered_model_name=model_name,\n",
    "        pip_requirements=[\n",
    "            \"mlflow==\" + mlflow._version_,\n",
    "            \"langchain==\" + langchain. _ version_,\n",
    "            \"databricks-vectorsearch\",\n",
    "        ],\n",
    "        input_example=question,\n",
    "        signature=signature\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c9d2a0-9d62-45b3-ae7b-682b990dd153",
   "metadata": {},
   "source": [
    "## planning a compound AI system architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988a279e-ad10-4828-8c84-be81ab143353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "#Libararies to dev graphics\n",
    "echo 'Driver Installs...'\n",
    "apt-get install -y graphviz\n",
    "pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6840ea-7a73-49a1-a89a-482789cc191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multistage_html():\n",
    "    import re\n",
    "    from graphviz import Digraph\n",
    "    \n",
    "    dot Digraph('pt')\n",
    "    dot.attr(compound='true')\n",
    "    dot.graph_attr['rankdir'] = 'LR'\n",
    "    dot.graph_attr['splines'] = 'ortho'\n",
    "    dot.edge_attr.update(arrowhead='normal', arrowsize='1')\n",
    "    dot.attr('node', shape='rectangle')\n",
    "def component_link(component,\n",
    "                    ttip=''):\n",
    "    url = \"https://curriculum-dev.cloud.databricks.com\"\n",
    "    path = \"/\".join(dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split(\"/\") [:-1])\n",
    "    path path.replace(\" \", \"%20\")\n",
    "    return {'tooltip': ttip, 'href': f'{url}#workspace(path}/components/{component)', 'target': \"_blank\",\n",
    "    'width': \"1.5\"}\n",
    "with dot.subgraph(name='cluster_workflow') as w:\n",
    "    w.body.append('label=\"Model Serving\"')\n",
    "    w.body.append('style=\"filled\"')\n",
    "    w.body.append('color=\"#808080\"')\n",
    "    w.body.append('fillcolor=\"#F5F5F5\"')\n",
    "    \n",
    "    w.node('question', 'question', fillcolor='#FFD580', style='filled', shape='oval',\n",
    "        **component_link('question'))\n",
    "with w.subgraph(name='cluster_app') as a:\n",
    "    a.body.append('label=\"Compound_rag_app Class\"')\n",
    "    a.body.append('style=\"filled\"')\n",
    "    a.body.append('color=\"#808080\"*)\n",
    "    a.body.append('fillcolor=\"#DCDCDC\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef2a07-9df8-4a27-87ce-b82b4f92add6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(self, question: str) -> str:\n",
    "    search_result: Similarity SearchResult = self.run_search (question)\n",
    "    augmented_result: Tuple [SearchResult Augmented Content, ...] = self.run_augment(search)\n",
    "    context: str = self.run_get_context(augmented_result)\n",
    "    qa_result: QaModelResult = self.run_qa(question, context)\n",
    "    class=\"reserved\" return Similarity SearchResult = self.run_search(question)\n",
    "    return qa_result.get_answer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eca6987-daf1-4913-9c8b-f45d056a942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building Multi-stage AI systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905d8b1-41f6-46c0-90ec-fcd462265ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad96775-08bd-4fb7-abfa-9983998b91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Username: {DA.username}\")\n",
    "print(f\"Catalog Name: {DA.catalog_name}\")\n",
    "print(f\"Schema Name:{DA.schema_name}\")\n",
    "print(f\"Working Directory:{DA.paths.working_dir}\")\n",
    "print(f\"Dataset Location:{DA.paths.datasets}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1506e95c-a6dc-4117-86dd-f0a9738d662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template PromptTemplate.from_template(\"Tell me about a (genre) movie which (actor) is one of the actors.\")\n",
    "prompt_template.format(genre=\"romance\", actor \"Brad Pitt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91375835-589e-400c-bb9c-93df0d1e42c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "\n",
    "# play with max_tokens to define the length of the response\n",
    "llm_dbrx ChatDatabricks (endpoint=\"databricks-dbrx-instruct\", max_tokens 500)\n",
    "\n",
    "for chunk in llm_dbrx.stream(\"Who is Brad Pitt?\"):\n",
    "    print(chunk.content, end=\"\\n\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c808f58-3777-412a-bceb-17d92021514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever\n",
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "retriever = WikipediaRetriever()\n",
    "#docs = retriever.get_relevant_documents (query=\"Brad Pitt\")\n",
    "docs retriever.invoke(input=\"Brad Pitt\")\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43794cab-a910-4aab-a183-aef2c0f8d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "tool = YouTube Search Tool()\n",
    "tool.run(\"Brad Pitt movie trailer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8051a59-97b4-41a3-b54f-190def4d1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tool.description)\n",
    "print(tool.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68036331-1745-4af5-aa75-fd5c35d1847a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chaining\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt_template | Illm_dbrx | StroutputParser()\n",
    "print(chain.invoke({\"genre\":\"romance\", \"actor\":\"Brad Pitt\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edf76e0-ba36-44a8-8051-7beb17237dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a multi-stage chain\n",
    "# create a vector store\n",
    "#assign vs search endpoint by username\n",
    "vs_endpoint_prefix = \"vs_endpoint_\"\n",
    "vs_endpoint_fallback = \"vs_endpoint_genai_as\"\n",
    "vs_endpoint_name vs_endpoint_prefix+str(get_fixed_integer(DA.unique_name(\"_\")))\n",
    "print(f\"Vector Endpoint name: (vs_endpoint_name). In case of any issues, replace variable 'vs_endpoint_name with 'vs_endpoint_fallback in demos and labs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ab365-99ca-466a-b8b2-35056abf4439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "vs_index_table_fullname = f\"{DA.catalog_name}.{DA.schema_name}.dais_embeddings\"\n",
    "source_table_fullname = f\"(DA.catalog_name}.{DA.schema_name}.dais_text\"\n",
    "\n",
    "#load dataset and compute embeddings\n",
    "df spark.read.parquet(f\" (DA.paths.datasets}/dais/dais23_talks.parquet\")\n",
    "dfdf.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "#df df.withColumn(\"embedding\", get_embedding(\"Abstract\"))\n",
    "df.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(source_table_fullname)\n",
    "\n",
    "spark.sql(f\"ALTER TABLE {source_table_fullname) SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "#store embeddings in vector store\n",
    "create_vs_index(vs_endpoint_name, vs_index_table_fullname, source_table_fullname, \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f87beb3-c898-4b85-8279-48bf4d8ea689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build first chain\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StroutputParser\n",
    "from langchain_community.tools import YouTube Search Tool\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm_dbrx= ChatDatabricks (endpoint=\"databricks-dbrx-instruct\", max_tokens = 1000)\n",
    "tool_yt = YouTube Search Tool()\n",
    "\n",
    "prompt_template_1 = PromptTemplate.from_template(\n",
    "    \"\"\"You are a Databricks expert. You will get questions about Databricks. Try to give simple answers and be professional.\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "chain1=({\"question\": RunnablePassthrough()} | prompt_template_1 | llm_dbrx | StroutputParser())\n",
    "print(chain1.invoke({\"question\":\"How machine learning models are stored in Unity Catalog?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d0451-7ea9-41c4-b7c2-65a262545586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build second chain\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "vsc VectorSearchClient()\n",
    "dais_index vsc.get_index(vs_endpoint_name, vs_index_table_fullname)\n",
    "query \"how do I use DatabricksSQL\"\n",
    "\n",
    "dvs_delta_sync = DatabricksVectorSearch(dais_index)\n",
    "docs = dvs_delta_sync.similarity_search (query)\n",
    "videos= tool_yt.run(docs[0].page_content)\n",
    "prompt_template_2 = PromptTemplate.from_template(\n",
    "    \"\"\"You will get a list of videos related to the user's question which are recorded in DAIS-2023. Encourage the user to watch the videos.\n",
    "    List videos with their YouTube links.\n",
    "    \n",
    "    List of videos: (videos)\n",
    "    \"\"\"\n",
    "chain2= ({\"videos\": RunnablePassthrough()) | prompt_template_2 | llm_dbrx | StroutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc20d44-425a-41e8-a299-08d2f4a6b571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "multi_chain = ({\n",
    "    \"c\": chain1,\n",
    "    \"d\": chain2\n",
    "}| RunnablePassthrough.assign(d=chain2))\n",
    "multi_chain.invoke({\"question\": \"How machine learning models are stored in Unity Catalog?\", \"videos\": videos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d684b4-2f0f-42b3-a13b-1bf3e34aabc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the brain of the agent\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "# play with max_tokens to define the length of the response\n",
    "llm_dbrx ChatDatabricks (endpoint=\"databricks-dbrx-instruct\", max_tokens 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec425233-cfc5-4279-9b94-66620462d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "#Wiki tool for info retrieval\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "tool wiki=WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "\n",
    "#tool to search youtube videos.\n",
    "tool_youtube =YouTubeSearchTool()\n",
    "\n",
    "#web search tool\n",
    "search =DuckDuckGoSearchRun()\n",
    "\n",
    "#tool to write python code\n",
    "python_repl =PythonREPL()\n",
    "repl_tool Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output\n",
    "    of a value, you should print it out with 'print(...)\".\",\n",
    "    func=python_repl.run\n",
    ")\n",
    "# toolset\n",
    "tools=[tool_wiki, tool_youtube, search, repl_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58265ed-d204-4fc6-aa33-29ac286d7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define planning logic\n",
    "from langchain.prompts import PromptTemplate\n",
    "template='''Answer the following questions as best you can. You have access to the following tools:\n",
    "{tools}\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}'''\n",
    "prompt= PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa0248-35e9-415c-b2f1-ac56b132a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Agent Executor\n",
    "from langchain.agents.react.agent import create_react_agent\n",
    "\n",
    "agent=create_react_agent(llm_dbrx, tools, prompt)\n",
    "brixo = AgentExecutor (agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "brixo.invoke({\"input\":\n",
    "    \"\"\"What would be a nice movie to watch in rainy weather. Follow these steps.\n",
    "    \n",
    "    First, decide which movie you would recommend.\n",
    "    \n",
    "    Second, show me the trailler video of the movie that you suggest.\n",
    "    \n",
    "    Next, collect data about the movie using search tool and draw a bar chart using Python libraries. If you can't find latest data use some dummy data as we to show your abilities to the learners. Don't use for python code. Input should be sanitized by removing any leading or trailing backticks. if the input starts with \"python\", remove that word as well. The output must be the result of executed code.\n",
    "    \n",
    "    Finally, tell a funny joke about agents.\n",
    "    \"\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ef5487-1de3-4901-a562-0b6fcccb9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset load_dataset(\"maharshipandya/spotify-tracks-dataset\")\n",
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad031a67-8e9e-4972-af9e-e058bd621d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "\n",
    "llm_dbrx ChatDatabricks (endpoint=\"databricks-dbrx-instruct\", max_tokens = 500)\n",
    "\n",
    "prefix=\"\"\"Input should be sanitized by removing any leading or trailing backticks. if the input starts with \"python\", remove that word as well. Use the dataset provided. The output must start with a new line.\"\"\"\n",
    "dataqio=create_pandas_dataframe_agent(\n",
    "    llm_dbrx\n",
    "    df,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    prefix=prefix,\n",
    "    agent_executor_kwargs={\n",
    "    \"handle_parsing_errors\": True\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa9f89-f1cf-447f-8d5d-967d30e612cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataqio.invoke(\"what is the artist name of most populat country song?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771b918-d2ff-4462-8c2f-87f2edecca56",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataqio.invoke(\"what is the total number of rows?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
