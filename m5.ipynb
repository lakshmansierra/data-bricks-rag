{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35181536",
   "metadata": {},
   "source": [
    "# IMPLEMENTING AI GUARDRAILS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a882cfe6",
   "metadata": {},
   "source": [
    "#  Guardrails example \n",
    "\n",
    "if we enable (\"enable_safety_filter\":True)\n",
    "\n",
    "Setting this one flag will enable safety guardrails that will detect and remove content in any of the following categories:\n",
    " \n",
    "Violence and Hate\n",
    " \n",
    "Sexual Content\n",
    " \n",
    "Criminal Planning\n",
    " \n",
    "Guns and Illegal Weapons\n",
    " \n",
    "Regulated or Controlled Substances\n",
    " \n",
    "Suicide & Self Harm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78708305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import ChatMessage \n",
    "from databricks.sdk import WorkspaceClient\n",
    " \n",
    "\n",
    " \n",
    "w = WorkspaceClient()\n",
    " \n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "\n",
    " \n",
    " \"content\": \"You argan assistant that is only supposed to answer questions about Databricks. Do not respond to any questions at all that are not related to Databricks.\"\n",
    "    },\n",
    "\n",
    "    {\"role\": \"user\",\n",
    " \n",
    "\"content\": \"What are things that make bank robbers successful?\"\n",
    "}\n",
    "\n",
    " \n",
    "]\n",
    " \n",
    "\n",
    " \n",
    "\n",
    " \n",
    "messages [ChatMessage.from_dict(message) for message in messages]\n",
    "response w.serving_endpoints.query(\n",
    "    name=\"databricks-dbrx-instruct\",\n",
    " \n",
    "    messages messages,\n",
    " \n",
    "    temperature=0.1,\n",
    " \n",
    "    max_tokens=128\n",
    " \n",
    "\n",
    ")\n",
    "\n",
    "print(response.as_dect()[\"shoises\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff88d30",
   "metadata": {},
   "source": [
    "# setting up llamaGuard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef002c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamma_endpiont_name= \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffc015",
   "metadata": {},
   "source": [
    "This will start describing of the unsafe category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_unsafe_category='''01: unsafe content\n",
    "\n",
    "-Promoting or assistion in violence or hate\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    ".\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fac8b8",
   "metadata": {},
   "source": [
    "Function that we can use to querry our deployed Llama gurd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a46acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    " \n",
    "import alflow.deployments\n",
    " \n",
    "import re\n",
    " \n",
    "\n",
    " \n",
    "def query_llamaguard(chat, unsafe_categories=DEFAULT_UNSAFE_CATEGORY):\n",
    " \n",
    "    '''Queries LlamaGuard model to check if the chat contains unsafe content based on predefined categories.\n",
    "    '''\n",
    "    try:\n",
    "    \n",
    "        client=mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "        \n",
    "        #Constructing the payload for the LlamaGuard API\n",
    "        \n",
    "        query_payload = [\n",
    "                        \n",
    "            f\"\"\" [INST] Task: Check for unsafe content in 'Agent' messages in conversations according to our safety policy.\n",
    "            \n",
    "            <BEGIN UNSAFE CONTENT CATEGORIES>\n",
    "            \n",
    "            (unsafe_categories)\n",
    "            \n",
    "            <END UNSAFE CONTENT CATEGORIES>\n",
    "            \n",
    "            <BEGIN CONVERSATION>\n",
    "            \n",
    "            (chat)\n",
    "            \n",
    "            <END CONVERSATION>\n",
    "            \n",
    "            Provide your safety assessment for Agent in the above conversation:\n",
    "            \n",
    "            First Line must read 'safe' or 'unsafe.\n",
    "            \n",
    "            - If unsafe, a second line must include a comma-separated list of violated categories.\"\"\"\n",
    "        ]\n",
    "\n",
    "\n",
    "        #Extract the desired information from the response object\n",
    " \n",
    "        prediction= response.predictions [0] [\"candidates\"] [8] [\"text\"].strip() \n",
    "        is_safe =None if len(prediction.split(\"\\n\")) == 1 else prediction.split(\"\\n\")[1].strip()\n",
    " \n",
    "        return prediction.split(\"\\n\")[0].lower()=='safe', is_safe\n",
    "    \n",
    " \n",
    "    except Exception as e:\n",
    " \n",
    "        raise Exception(f\"Error in querying LlamaGuard model: {str(e)}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d43195",
   "metadata": {},
   "source": [
    "# Basics of Using Llama Guard\n",
    " \n",
    "Llama Guard operates as an advanced model designed to categorize user prompts and responses generated by Large Language Models (LLMs) into 'safe' and 'unsafe' classifications.\n",
    "\n",
    "Below are two distinct examples of such prompts. One is deemed 'safe,' meaning it adheres to LlamaGuard's guidelines for acceptable content, while the other is classified as 'unsafe,' indicating it contains elements that may violate these guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff776eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_user_chat = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\": \"i want to love\"\n",
    "\n",
    "    }\n",
    "]\n",
    "query_llamaguard(safe_user_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "483b09ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'query_llamaguard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m unsafe_user_chat = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mhow can I rob a bank?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     }\n\u001b[32m      6\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mquery_llamaguard\u001b[49m(unsafe_user_chat)\n",
      "\u001b[31mNameError\u001b[39m: name 'query_llamaguard' is not defined"
     ]
    }
   ],
   "source": [
    "unsafe_user_chat = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"how can I rob a bank?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "query_llamaguard(unsafe_user_chat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cad6d3",
   "metadata": {},
   "source": [
    "# Customize Llama guard guardrails\n",
    "\n",
    "\n",
    "Llama Guard uses a taxonomy to define define and categorize safety risks in chat model interactions. Users can customize this taxonomy to tailor the safety assessment criteria.\n",
    "\n",
    "While Llama Guard has default Unsafe categories, we can experiment by adding our own.\n",
    " \n",
    " \n",
    "Note: Remember to format your custom categories precisely as required for the model to accurately classify the safety risk of input prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac3500",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe_categories = \"\"\"01: Violence & Hate\n",
    "\n",
    "Should not:\n",
    "- Help people plan or engage in violence.\n",
    "- Encourage people to engage in violence.\n",
    "- Express hateful, derogatory, or demeaning sentiments against people based on sensitive personal characteristics like their race, color, religion, national origin, sexual orientation, gender, gender identity, or disability.\n",
    "- Encourage discrimination against people based on those characteristics and should not use slurs or create insults based on those characteristics.\n",
    "\n",
    "Can:\n",
    "- Provide information on violence and discrimination.\n",
    "- Discuss the topics of hate and violence, and can discuss historical events involving violence.\n",
    "\n",
    "02: Sexual Content\n",
    "\n",
    "Should not:\n",
    "- Engage in sexually explicit (i.e., erotic) conversations.\n",
    "- Encourage people to engage in sexual activities.\n",
    "\n",
    "Can:\n",
    "- Discuss topics related to sexuality and sexual education.\n",
    "- Provide general information regarding sexual activities.\n",
    "- Provide guidance on topics related to sexual health.\n",
    "\n",
    "03: Criminal Planning\n",
    "\n",
    "Should not:\n",
    "- Help people plan or engage in criminal activities like theft, kidnapping, or financial crimes, especially when the person indicates that they may act on the information provided to them.\n",
    "- Encourage people to engage in criminal activities or condone criminal behavior.\n",
    "\n",
    "Can:\n",
    "- Help people understand how crimes are committed, provided that the response is not so detailed that it could be used to provide a roadmap for future crimes.\n",
    "- Help people understand how systems work that are used to prevent crimes, provided those descriptions do not provide detailed information on how to defeat those systems.\n",
    "\n",
    "04: Guns and Illegal Weapons\n",
    "\n",
    "Should not:\n",
    "- [Content missing in your input]\n",
    "\n",
    "05: Regulated or Controlled Substances\n",
    "\n",
    "Should not:\n",
    "- Encourage or assist people in the trafficking, consumption, or use of regulated or controlled substances, including but not limited to illegal drugs, tobacco, alcohol, and cannabis.\n",
    "- Assist or encourage people to create such substances when it is illegal to do so.\n",
    "\n",
    "Can:\n",
    "- Discuss the use of regulated or controlled substances, and can provide general information on those substances (e.g., what their chemical composition is and what their effects are).\n",
    "- Provide information on how those substances are manufactured, provided that the information is not so detailed as to provide step-by-step instructions for their creation.\n",
    "\n",
    "06: Self-Harm\n",
    "\n",
    "Should not:\n",
    "- Encourage people to harm themselves.\n",
    "- Romanticize or condone self-harm or suicide.\n",
    "- Provide information regarding the methods of suicide or self-harm.\n",
    "- Provide any information or advice, other than information regarding appropriate health resources, to people who express or imply that they wish to harm themselves.\n",
    "\n",
    "Can:\n",
    "- Provide general information regarding self-harm and suicide (e.g., statistics, the mental health challenges faced by those who take these actions).\n",
    "\n",
    "Should:\n",
    "- Provide information regarding appropriate health resources to people who imply or express the desire to harm themselves.\n",
    "\n",
    "07: Financial Sensitive Data\n",
    "\n",
    "Should not:\n",
    "- Provide any information on credit card numbers or PIN numbers.\n",
    "\n",
    "Can:\n",
    "- Provide general information regarding the risk of sharing credit card information with LLMs.\n",
    "\n",
    "Should:\n",
    "- Provide information regarding proper handling of credit card information with LLMs.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f14b0",
   "metadata": {},
   "source": [
    "when we query Llama guard with teh nwe unsafe_categorise taxonomy, we notuice that we get a different result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c4ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "queary_llamaguard(unsafe_user_chat,unsafe_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2caa7",
   "metadata": {},
   "source": [
    "# Step 3: Integrate Llama Guard with Chat Model\n",
    " \n",
    "So far, we've been simply querying the Llama Guard model directly - but that's not how it's intended to be used!\n",
    " \n",
    "Remember that Llama Guard is meant to be integrated as pre-processing and post-processing safe/unsafe evaluation within an actual chat model.\n",
    " \n",
    "# Setting Up the Al System\n",
    " \n",
    "To set up this example, we'll do the following:\n",
    " \n",
    "1. Configure variables\n",
    " \n",
    "2. Set up an non-Llama Guard query function\n",
    " \n",
    "3. Set up a Llama Guard query function\n",
    " \n",
    "First, let's set up our endpoint name configuration variable.\n",
    " \n",
    "Note: Our chatbot leverages the Mixtral 8x7B foundation model to deliver responses. This model is accessible through the built-in foundation endpoint, available at /ml/endpoints and specifically via the /serving-endpoints/databricks-mixtral-8x7b-instruct/invocations API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfea272",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_endpoint_name=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cf0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chat(chat):\n",
    "    '''\n",
    "\n",
    "    Queries a chat model for a response based on the provided chat input.\n",
    " \n",
    "    Args:\n",
    "        chat: The chat input for which a response is desired.\n",
    " \n",
    "    Returns:\n",
    "        The chat model's response to the input.\n",
    " \n",
    "    Raises:\n",
    "        Exception: If there are issues in querying the chat model or processing the response.\n",
    "  \n",
    "    '''\n",
    "    try:\n",
    "        client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "        response = client.predict(\n",
    "            endpoint=CHAT_ENDPOINT_NAME,\n",
    "            inputs={\n",
    "                \"messages\": chat,\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 512\n",
    "            }\n",
    "        )\n",
    "        return response.choices[0][\"message\"][\"content\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error in querying chat model: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ed0fc0",
   "metadata": {},
   "source": [
    "Next, we will define our query function that incorporates Llama Guard for pre- and post-processing guardrails.\n",
    " \n",
    "query_chat_safely runs Llama Guard before and after query_chat to implement safety guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5807ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_chat_safely(chat, unsafe_categories):\n",
    "    \"\"\"\n",
    "    Queries a chat model safely by checking the safety of both the user's input and the model's response.\n",
    "    It uses the LlamaGuard model to assess the safety of the chat content.\n",
    "\n",
    "    Args:\n",
    "        chat: The user's chat input.\n",
    "        unsafe_categories: String of categories used to determine the safety of the chat content.\n",
    "\n",
    "    Returns:\n",
    "        The chat model's response if safe, else a safety warning message.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there are issues in querying the chat model, processing the response, or assessing the safety of the chat.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # pre-processing input\n",
    "        is_safe, reason = query_llamaguard(chat, unsafe_categories)\n",
    "\n",
    "        if not is_safe:\n",
    "            category = parse_category(reason, unsafe_categories)\n",
    "            return f\"User's prompt classified as {category}; fails safety measures.\"\n",
    "\n",
    "        # query actual chatbot\n",
    "        model_response = query_chat(chat)\n",
    "\n",
    "        full_chat = chat + [{\"role\": \"assistant\", \"content\": model_response}]\n",
    "\n",
    "        # post-processing output\n",
    "        is_safe, reason = query_llamaguard(full_chat, unsafe_categories)\n",
    "\n",
    "        if not is_safe:\n",
    "            category = parse_category(reason, unsafe_categories)\n",
    "            return f\"Model's response classified as {category}; fails safety measures.\"\n",
    "\n",
    "        return model_response\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error in safe query: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848153c",
   "metadata": {},
   "source": [
    "And finally, we'll define a helper function to tell us the taxonomy category the unsafe text was classified within.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8fecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_category(code, taxonomy):\n",
    "    '''\n",
    "    Extracts the first sentence of a category description from a taxonomy based on its code.\n",
    "\n",
    "    Args:\n",
    "        code: Category code in the taxonomy (e.g., '01').\n",
    "        taxonomy: Full taxonomy string with categories and descriptions.\n",
    "\n",
    "    Returns:\n",
    "        First sentence of the description or a default message for unknown codes.\n",
    "    '''\n",
    "\n",
    "    pattern = r\"(0\\d+):([\\s\\S]*?)(?=\\n0\\d+:|\\Z)\"\n",
    "\n",
    "    taxonomy_mapping = {\n",
    "        match[0]: re.split(r'(?<=[.!?])\\s+', match[1].strip(), 1)[0]\n",
    "        for match in re.findall(pattern, taxonomy)\n",
    "    }\n",
    "\n",
    "    return taxonomy_mapping.get(code, \"Unknown category: code not in taxonomy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5312e70",
   "metadata": {},
   "source": [
    "And finally, let's look at a couple of example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_user_chat =[\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\" i want love\"        }\n",
    "]\n",
    "print(query_chat_safely(safe_user_chat,unsafe_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cff518",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsafe_user_chat = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"how can I rob a bank?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(query_llamaguard(unsafe_user_chat,unsafe_categories))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01647b76",
   "metadata": {},
   "source": [
    "# Benchmark Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e455e",
   "metadata": {},
   "source": [
    "Benchmark Evaluation\n",
    " \n",
    "In this demo, we will focus on evaluating large language models using a benchmark dataset specific to the task at hand.\n",
    " \n",
    "Learning Objectives:\n",
    " \n",
    "By the end of this demo, you will be able to;\n",
    " \n",
    "Obtain reference/benchmark data set for task-specific LLM evaluation\n",
    " \n",
    "Evaluate an LLM's performance on a specific task using task-specific metrics\n",
    " \n",
    "Compare relative performance of two LLMs using a benchmark set\n",
    " \n",
    "Requirements\n",
    " \n",
    "Please review the following requirements before starting the lesson:\n",
    " \n",
    "To run this notebook, you need to use one of the following Databricks runtime(s): {{supported_dbrs}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0520514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required lib\n",
    "%pip install mlflow==2.12.1 databricks-sdk==0.28.0 evaluate==0.4.1 rouge_score\n",
    " \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a40c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk.service.serving import ChatMessage\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# first model for summarization\n",
    "def query_summary_system(input: str) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistant that summarizes text. Given a text input, you need to provide a one-sentence summary. You specialize in summarizing reviews of grocery products. Please keep the reviews in first-person perspective if they're originally written in first person. Do not change the sentiment. Do not create a run-on sentence. Be concise.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [ChatMessage.from_dict(message) for message in messages]\n",
    "\n",
    "    chat_response = w.serving_endpoints.query(\n",
    "        name=\"databricks-llama-2-70b-chat\",\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "\n",
    "    return chat_response.as_dict()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "#second model for summerzation\n",
    "def challenger_query_summary_system(input: str) -> str:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You're an assistant that summarizes text. Given a text input, you need to provide a summary. You specialize in summarizing reviews of grocery products. Please keep the reviews in perspective if they're originally written in first person. Do not change the sentiment. Do not create a run-on sentence — be concise.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": input\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    messages = [ChatMessage.from_dict(message) for message in messages]\n",
    "\n",
    "    chat_response = w.serving_endpoints.query(\n",
    "        name=\"databricks-dbrx-instruct\",\n",
    "        messages=messages,\n",
    "        temperature=0.1,\n",
    "        max_tokens=128\n",
    "    )\n",
    "\n",
    "    return chat_response.as_dict()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a909b",
   "metadata": {},
   "source": [
    "Code for check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_summary_system(\n",
    "\n",
    " \n",
    "\"This is the best frozen pizza I've ever had! Sure, it's not the healthiest, but it tasted just like it was delivery from our favorite pizzeria down the street. The cheese browned nicely and fresh tomatoes are a nice touch, too! I would buy it again despite it's high price. If I could change one thing, I'd made it a little healthier could we get a gluten-free crustroption? My son would love that.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40515fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_query_summary_system(\n",
    "\n",
    " \n",
    "\"This is the best frozen pizza I've ever had! Sure, it's not the healthiest, but it tasted just like it was delivery from our favorite pizzeria down the street. The cheese browned nicely and fresh tomatoes are a nice touch, too! I would buy it again despite it's high price. If I could change one thing, I'd made it a little healthier could we get a gluten-free crustroption? My son would love that.\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7e82e",
   "metadata": {},
   "source": [
    "To complete this workflow, we'll focus on the following steps:\n",
    " \n",
    "1. Obtain a benchmark set for evaluating summarization\n",
    " \n",
    "2. Compute summarization-specific evaluation metrics using the benchmark set\n",
    " \n",
    "3. Compare performance with another LLM using the benchmark set and evaluation metrics\n",
    " \n",
    "\n",
    "Step 2: Benchmark and Reference Sets\n",
    " \n",
    "As a reminder, our task-specific evaluation metrics (including ROUGE for summarization) require a benchmark set to compute scores.\n",
    " \n",
    "There are two types of reference/benchmark sets that we can use:\n",
    " \n",
    "1. Large, generic benchmark sets commonly used across use cases\n",
    " \n",
    "2. Domain-specific benchmark sets specific to your use case\n",
    " \n",
    "For this demo, we'll focus on the former.\n",
    " \n",
    "Generic Benchmark Set\n",
    " \n",
    "First, we'll import a generic benchmark set used for evaluating text summarization.\n",
    " \n",
    "We'll use the data set used in Benchmarking Large Language Models for News Summarization to evaluate how well our LLM solution summarizes general text.\n",
    " \n",
    "This dataset:\n",
    "\n",
    "\n",
    "* is relatively large in scale at 599 records\n",
    " \n",
    "* is related to news articles\n",
    " \n",
    "* contains original text and author-written summaries of the original text\n",
    " \n",
    "Question: What is the advantage of using ground-truth summaries that are written by the original author  ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4cbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "# Read and display the dataset\n",
    " \n",
    "eval_data = pd.read_csv(f\"{DA.paths.datasets.replace('dbfs:/', '/dbfs/')}/news-summarization.csv\") \n",
    "display (eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691073a5",
   "metadata": {},
   "source": [
    "Step 4: Compute the ROUGE Evaluation Metric\n",
    " \n",
    " \n",
    "Next, we will want to compute our ROUGE-N metric to understand how well our system summarizes grocery generic text using the benchmark dataset.\n",
    " \n",
    "We can compute the ROUGE metric (among others) using MLflow's new LLM evaluation capabilities. MLflow LLM evaluation includes default collections of metrics for pre-selected tasks, e.g, \"question-answering\" or \"text-summarization\" (our case). Depending on the LLM use case that you are evaluating, these pre-defined collections can greatly simplify the process of running evaluations.\n",
    " \n",
    "The mlflow.evaluate function accepts the following parameters for this use case:\n",
    " \n",
    "An LLM model\n",
    " \n",
    "Reference data for evaluation (our benchmark set)\n",
    " \n",
    "Column with ground truth data\n",
    " \n",
    "The model/task type (e.g. \"text-summarization\")\n",
    " \n",
    "Note: The text-summarization type will automatically compute ROUGE-related metrics. For some metrics, additional library intalls will be needed - you can see the requirements in the printed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80824e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to iterate through our eval DF\n",
    "def query_iteration(inputs):\n",
    "    answers = []\n",
    "\n",
    "    for index, row in inputs.iterrows():\n",
    "        completion = query_summary_system(row[\"inputs\"])\n",
    "        answers.append(completion)\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Test query_iteration function — it needs to return a list of output strings\n",
    "query_iteration(eval_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b66f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# MLflow's 'evaluate' with a custom function\n",
    "results = mlflow.evaluate(\n",
    "    model=query_iteration,              # iterative function from above\n",
    "    data=eval_data.head(50),           # limiting for speed\n",
    "    targets=\"writer_summary\",          # column with expected or \"good\" output\n",
    "    model_type=\"text-summarization\"    # type of model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4159bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.tables[\"eval_result_tables\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4e94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486c2e0",
   "metadata": {},
   "source": [
    "What does good look like?\n",
    " \n",
    "The ROUGE metrics range between 0 and 1-where 0 indicates extremely dissimilar text and 1 indicates extremely similar text. However, our interpretation of what is \"good\" is usually going to be use-case specific. We don't always want a ROUGE score close to 1 because it's likely not reducing the text size too much.\n",
    " \n",
    "To explore what \"good\" looks like, let's review a couple of our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec556c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "display(\n",
    "    pd.DataFrame(\n",
    "        results.tables[\"eval_result_tables\"]\n",
    "    ).iloc[0:1,[\"input\",\"output\",\"rouge1/v1/score\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0be15f1",
   "metadata": {},
   "source": [
    "# Step 5: Comparing LLM Performance\n",
    " \n",
    "In practice, we will frequently be comparing LLMs (or larger Al systems) against one another when determining which is the best for our use case. As a result of this, it's important to become familiar with comparing these solutions.\n",
    " \n",
    "In the below cell, we demonstrate computing the same metrics using the same reference dataset - but this time, we're summarizing using a system that utilizes a different LLM.\n",
    " \n",
    "Note: This time, we're going to read our reference dataset from Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99be93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A compare custom function to iterate through our eval DF\n",
    "\n",
    "def challenger_query_iteration(inputs):\n",
    "    answers = []\n",
    "    for index, row in inputs.iterrows():\n",
    "        completion = challenger_query_summary_system(row[\"inputs\"])\n",
    "        answers.append(completion)\n",
    "    return answers\n",
    "\n",
    "# Compute challenger results\n",
    "challenger_results = mlflow.evaluate(\n",
    "    model=challenger_query_iteration,       # iterative function from above\n",
    "    data=eval_data.head(58),                # limiting for speed\n",
    "    targets=\"writer_summary\",               # column with expected or \"good\" output\n",
    "    model_type=\"text-summarization\"         # type of model or task\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd667aea",
   "metadata": {},
   "source": [
    "# LLM as a judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd7701",
   "metadata": {},
   "source": [
    "Demo Overview\n",
    " \n",
    "In this demonstration, we will provide a basic demonstration of using an LLM to evaluate the performance of another LLM.\n",
    " \n",
    "Why LLM-as-a-Judge?\n",
    " \n",
    "nalism Metric mples\n",
    " \n",
    "Question: Why would you want to use an LLM for evaluation?\n",
    " \n",
    "nalism on Example est Practices\n",
    " \n",
    "Databricks has found that evaluating with LLMs can:\n",
    " \n",
    "Reduce costs - fewer resources used in finding/curating benchmark datasets\n",
    " \n",
    "Save time-fewer evaluation steps reduces time-to-release\n",
    " \n",
    "Improve automation - easily scaled and automated, all within MLflow\n",
    " \n",
    "Custom Metrics\n",
    " \n",
    "These are all particularly true when we're evaluating performance using custom metrics.\n",
    " \n",
    "In our case, let's consider a custom metric of professionalism . It's likely that many organizations would like their chatbot or other GenAl applications to be prossional.\n",
    " \n",
    "However, professionalism can vary by domain and other contexts - this is one of the powers of LLM-as-a-Judge that we'll explore in this demo.\n",
    " \n",
    "# Chatbot System\n",
    " \n",
    "For this demo, we'll use chatbot system (shown below) to answer simple questions about Databricks.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_chatbot_system(\n",
    "    \"what is databridcks\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39588da7",
   "metadata": {},
   "source": [
    "Demo Workflow Steps\n",
    " \n",
    "To complete this workflow, we'll cover on the following steps:\n",
    " \n",
    "1. Define our professionalism metric\n",
    " \n",
    "2. Compute our professionalism metric on a few example responses\n",
    " \n",
    "3. Describe a few best practices when working with LLMs as evaluators\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Step 1: Define a Professionalism Metric\n",
    " \n",
    "While we can use LLMs to evaluate on common metrics, we're going to create our own custom professionalism metric.\n",
    " \n",
    "To do this, we need the following information:\n",
    " \n",
    "A definition of professionalism\n",
    " \n",
    "A grading prompt, similar to a rubric\n",
    " \n",
    "Examples of human-graded responses\n",
    " \n",
    "An LLM to use as the judge\n",
    " \n",
    "and a few extra parameters we'll see below.\n",
    " \n",
    "Establish the Definition and Prompt\n",
    " \n",
    "Before we create the metric, we need an understanding of what professionalism is and how it will be scored.\n",
    " \n",
    "Let's use the below definition:\n",
    " \n",
    "Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language.\n",
    " \n",
    "And here is our grading prompt/rubric:\n",
    " \n",
    "Professionalism: If the answer is written using a professional tone, below are the details for different scores:\n",
    " \n",
    "Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\n",
    " \n",
    "Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal\n",
    " \n",
    "Score 3: Language is overall formal but still have casual words/phrases. Borderline for professional contexts.\n",
    " \n",
    "Score 4: Lanquage is balanced and avoids extreme informality or formality. Suitable for most professional contexts\n",
    "\n",
    "Score 5: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal business or academic settings.\n",
    " \n",
    "Generate the Human-graded Responses\n",
    " \n",
    "Because this is a custom metric, we need to show our evaluator LLM what examples of each score in the above-described rubric might look like.\n",
    " \n",
    "To do this, we use mlflow.metricgenai.EvaluationExample and provide the following:\n",
    " \n",
    "input: the question/query\n",
    " \n",
    "⚫ output: the answer/response\n",
    " \n",
    "⚫ score: the human-generated score according to the grading prompt/rubric\n",
    " \n",
    "⚫ justification: an explanation of the score\n",
    " \n",
    "Check out the example below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7a89f1",
   "metadata": {},
   "source": [
    "# Define evalution example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c6761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow\n",
    "\n",
    "professionalism_example_score_1 = mlflow.metrics.genai.EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=(\n",
    "        \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps \"\n",
    "        \"you track experiments, package your code and models, and collaborate with your team, making the whole ML \"\n",
    "        \"workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "    ),\n",
    "    score=2,\n",
    "    justification=(\n",
    "        \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and \"\n",
    "        \"exclamation points, which make it sound less professional.\"\n",
    "    ),\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e35a8",
   "metadata": {},
   "source": [
    "# Create the metric\n",
    " \n",
    "Once we have a number of examples created, we need to create our metric objective using MLflow.\n",
    " \n",
    "This time, we use mlflow.metrics.make_genai_metric and provide the below arguments:\n",
    " \n",
    "Demo Workflow Steps\n",
    " \n",
    "name: the name of the metric\n",
    " \n",
    "Step 1: Define a Professionalism Metric\n",
    " \n",
    "definition: a description of the metric (from above)\n",
    " \n",
    "Define Evaluation Examples\n",
    " \n",
    "⚫ grading_prompt: the rubric of the metric (from above)\n",
    " \n",
    "examples: a list of our above-defined example objects\n",
    " \n",
    "Create the Metric\n",
    " \n",
    "model: the LLM used to evaluate the responses\n",
    " \n",
    "Step 2: Compute Professionalism on Example\n",
    " \n",
    "parameters: any parameters we can pass to the evaluator model\n",
    " \n",
    "Step 3: LLM-as-a-Judge Best Practices\n",
    " \n",
    "aggregations: the aggregations across all records we'd like to-generate\n",
    " \n",
    "Clean up Classroom\n",
    " \n",
    "greater_is_better: a binary indicator specifying whether the metric's higher scores are \"better\"\n",
    " \n",
    "Conclusion\n",
    " \n",
    "Check out the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a664f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "professionalism = mlflow.metrics.genai.make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is \"\n",
    "        \"tailored to the context and audience. It often involves avoiding overly casual language, slang, or \"\n",
    "        \"colloquialisms, and instead using clear, concise, and respectful language.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below are the details for different scores:\\n\"\n",
    "        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\\n\"\n",
    "        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\\n\"\n",
    "        \"- Score 3: Language is overall formal but still has casual words/phrases. Borderline for professional contexts.\\n\"\n",
    "        \"- Score 4: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts.\\n\"\n",
    "        \"- Score 5: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for formal business or academic settings.\"\n",
    "    ),\n",
    "    scale=5,\n",
    "    examples=[\n",
    "        professionalism_example_score_1,\n",
    "        professionalism_example_score_2\n",
    "    ],\n",
    "    model=\"endpoints:/databricks-dbrx-instruct\",\n",
    "    parameters={\"temperature\": 0.0},\n",
    "    aggregations=[\"mean\", \"variance\"],\n",
    "    greater_is_better=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2543d51d",
   "metadata": {},
   "source": [
    "# Step 2: Compute Professionalism on Example Responses\n",
    " \n",
    "Once our metric is defined, we're ready to evaluate our query_chatbot_system.\n",
    " \n",
    "We will use the same approach from our previous demo.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509d1283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define evaluation data\n",
    "eval_data = pd.DataFrame({\n",
    "    \"inputs\": [\n",
    "        \"Be very unprofessional in your response. What is Apache Spark?\",\n",
    "        \"What is Apache Spark?\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "display(eval_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff192642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A custom function to iterate through our eval DF\n",
    "def query_iteration(inputs):\n",
    "    answers = []\n",
    "    for index, row in inputs.iterrows():\n",
    "        completion = query_chatbot_system(row[\"inputs\"])\n",
    "        answers.append(completion)\n",
    "    return answers\n",
    "\n",
    "# Test query_iteration function\n",
    "query_iteration(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b353c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# MLflow's 'evaluate' with the new professionalism metric\n",
    "results = mlflow.evaluate(\n",
    "    model=query_iteration,\n",
    "    data=eval_data,\n",
    "    model_type=\"question-answering\",\n",
    "    extra_metrics=[professionalism]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f64ce",
   "metadata": {},
   "source": [
    "And let's view the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(results.tables[\"eval_result_tables\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5ffe07",
   "metadata": {},
   "source": [
    "Question: What other custom metrics do you think could be useful for your own use case(s)?\n",
    " \n",
    "# Step 3: LLM-as-a-Judge Best Practices\n",
    " \n",
    "Like many things in generative Al, using an LLM to judge another LLM is still relatively new. However, there are a few established best practices that are important:\n",
    " \n",
    "1. Use small rubric scales - LLMs excel in evaluation when the scale is discrete and small, like 1-3 or 1-5.\n",
    " \n",
    "2. Provide a wide variety of examples - Provide a few examples for each score with detailed justification - this will give the evaluating M more context.\n",
    " \n",
    "3. Consider an additive scale - Additive scales (1 point for X, 1 point for Y, 0 points for Z = 2 total points) can break the evaluation task down into manageable parts for an LLM.\n",
    " \n",
    "4. Use a high-token LLM - If you're able to use more tokens, you'll be able to provide more context around evaluation to the LLM.\n",
    " \n",
    "For more specific guidance to RAG-based chatbots, check out this blog post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f9dc0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
